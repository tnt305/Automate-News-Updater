{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://console.groq.com/docs/rate-limits\n",
    "api_kay = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image showcases a close-up view of the central processing unit (CPU) of a video card or GPU, featuring a grid-like design. The rectangular, gold-colored bars with a glossy, metallic finish are arranged in a symmetrical pattern, with four vertical columns and ten horizontal rows.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=api_kay)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.2-11b-vision-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What's in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's technology landscape, and their importance can be seen in several areas:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user input, providing a seamless and interactive experience. This is particularly important for applications such as chatbots, virtual assistants, and language translation software.\n",
      "2. **Real-time Processing**: Fast language models can process and analyze large amounts of text data in real-time, allowing for applications such as sentiment analysis, entity recognition, and topic modeling to be performed quickly and efficiently.\n",
      "3. **Efficient Resource Utilization**: Fast language models can run on devices with limited computational resources, such as smartphones, smart home devices, and edge devices, making them ideal for applications where resources are constrained.\n",
      "4. **Scalability**: Fast language models can handle large volumes of text data and scale to meet the needs of growing applications, making them suitable for use cases such as text classification, language translation, and text generation.\n",
      "5. **Enhanced Accuracy**: Fast language models can be trained on large datasets and fine-tuned for specific tasks, leading to improved accuracy and performance in applications such as language translation, sentiment analysis, and text summarization.\n",
      "6. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by providing faster and more accurate language-based services, such as language translation, text analysis, and content generation.\n",
      "7. **Cost Savings**: Fast language models can reduce the computational resources required for language processing tasks, leading to cost savings and reduced energy consumption.\n",
      "8. **Increased Accessibility**: Fast language models can enable language-based services to be deployed in areas with limited internet connectivity or on devices with limited computational resources, increasing accessibility to these services.\n",
      "\n",
      "Some examples of applications that benefit from fast language models include:\n",
      "\n",
      "1. **Virtual assistants**: Fast language models enable virtual assistants to respond quickly to user queries and provide accurate results.\n",
      "2. **Language translation apps**: Fast language models enable language translation apps to translate text in real-time, allowing for seamless communication across languages.\n",
      "3. **Chatbots**: Fast language models enable chatbots to respond quickly to user input, providing a more interactive and engaging experience.\n",
      "4. **Sentiment analysis**: Fast language models can analyze large amounts of text data in real-time, allowing for sentiment analysis and opinion mining to be performed quickly and efficiently.\n",
      "5. **Text summarization**: Fast language models can summarize long documents and articles in real-time, providing a quick and accurate summary of the content.\n",
      "\n",
      "Overall, fast language models are essential for providing efficient, accurate, and scalable language-based services, and their importance will continue to grow as the demand for language-based applications increases.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=api_kay)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_completion_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
